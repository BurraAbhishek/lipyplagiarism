from nltk.tokenize import word_tokenize

def tokenize(sentence):
    """ Convert a sentence into words. """

    return word_tokenize(sentence)